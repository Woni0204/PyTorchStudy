{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyOBWupuC3O1pgd+Fxfj1cr7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["* Tensor는 PyTorch에서 중요한 추상 데이터 자료형.\n","* 이 interactive notebook은 torch.Tensor 클래스에 대한 심층적인 소개를 제공함."],"metadata":{"id":"MqFTK3HTmP2y"}},{"cell_type":"markdown","source":["* 먼저 가장 중요한 것은 PyTorch 모듈을 import 하는 것.\n","* 또한 몇 가지 예제에 사용할 math 모듈도 import 함."],"metadata":{"id":"e6Js8-iwmWPk"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"JN3M1KlqlXcE","executionInfo":{"status":"ok","timestamp":1710051942926,"user_tz":-540,"elapsed":3212,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}}},"outputs":[],"source":["import torch\n","import math"]},{"cell_type":"markdown","source":["### Tensor 생성하기\n","* tensor를 생성하는 가장 간단한 방법은 torch.empty() 를 호출하는 것."],"metadata":{"id":"vH913x8KLLlD"}},{"cell_type":"code","source":["x = torch.empty(3, 4)\n","print(type(x))\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NVEDQdBFLUKZ","executionInfo":{"status":"ok","timestamp":1710051942926,"user_tz":-540,"elapsed":17,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"09063837-aaff-4e85-aab7-b5061e58f540"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'torch.Tensor'>\n","tensor([[2.2421e-44, 2.7551e-40, 5.9372e-08, 3.2356e-41],\n","        [1.5961e-42, 6.3053e+26, 5.9372e-08, 3.2356e-41],\n","        [2.2421e-44, 2.8494e-14, 5.9372e-08, 3.2356e-41]])\n"]}]},{"cell_type":"markdown","source":["* 방금 한 내용은\n","  - torch 모듈에 있는 수많은 메소드 중 하나를 사용해서 tensfor를 만들었음.\n","  - 이 tensor는 3개의 행과 4개의 열을 가진 2차원 tensor.\n","  - 객체가 반환한 type은 torch.Tensor 이며, 이는 torch.FloatTensor 의 별칭임.\n","  - 기본적으로 PyTorch tensor는 32-bit 부동 소수점 표현 실수로 채워짐. (아래에서 더 많은 데이터 자료형을 소개함.\n","  - 생성한 tensor를 출력하면 아마 무작위 값을 볼 수 있을 것.\n","  - torch.empty()는 tensor를 위한 메모리를 할당해 주지만 임의의 값으로 초기화하지는 않음. (그렇기 때문에 할당 당시에 메모리가 가지고 있는 값을 보는 것.)"],"metadata":{"id":"qezGbl8TLXy5"}},{"cell_type":"markdown","source":["* 간략하게 tensor와 tensor의 차원 수, 그리고 tensor의 용어에 대해 알아보기.\n","  - 때로는 1차원 tensor를 보게 될 것인데 이는 vector.\n","  - 이와 마찬가지로 2차원 tensor는 주로 matrix.\n","  - 2차원보다 큰 차원을 가진 것들은 일반적으로 그냥 tensor."],"metadata":{"id":"sHGijYGBLXEh"}},{"cell_type":"markdown","source":["* 코딩하면서 주로 tensor를 몇 가지 값으로 초기화하고 싶을 수가 있음.\n","* 일반적인 경우로는 모두 0으로 초기화하거나, 모두 1로 초기화 하거나, 혹은 모두 무작위 값으로 초기화 할 때가 있는데, 이때 torch 모듈은 이 모든 경우를 위한 메소드를 제공."],"metadata":{"id":"55ob0NY_MVTg"}},{"cell_type":"code","source":["zeros = torch.zeros(2, 3)\n","print(zeros)\n","\n","ones = torch.ones(2, 3)\n","print(ones)\n","\n","torch.manual_seed(1729)\n","random = torch.rand(2, 3)\n","print(random)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c5EMir8iMvO9","executionInfo":{"status":"ok","timestamp":1710051942926,"user_tz":-540,"elapsed":16,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"d130ea73-093a-44e0-d278-6f6c9a611de2"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0., 0.],\n","        [0., 0., 0.]])\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]])\n","tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n"]}]},{"cell_type":"markdown","source":["* 이 모든 팩토리 메소드들은 우리가 기대하던 것들을 모두 수행함.\n","* 0으로 모두 채워 진 tensor, 1로 모두 채워 진 tensor 그리고 0과 1 사이의 무작위 값으로 채워 진 tensor를 얻음."],"metadata":{"id":"diwzVQahM6Az"}},{"cell_type":"markdown","source":["### 무작위 Tensor와 Seed 사용하기\n","* 무작위 tensor에 대해 말하자면, 바로 앞에서 호출하는 torch.manual_seed()를 눈치챘나요?\n","* 특히 연구 환경에서 연구 결과의 재현 가능성에 대한 확신을 얻고 싶을 때, 모델의 학습 가중치와 같은 무작위 값을 가진 tensor로 초기화 하는 것은 흔하거나 종종 일어나는 일.\n","* 직접 무작위 난수 생성기의 seed를 설정하는 것이 다음 방법."],"metadata":{"id":"5ytDqhdfNDNu"}},{"cell_type":"code","source":["torch.manual_seed(1729)\n","random1 = torch.rand(2, 3)\n","print(random1)\n","\n","random2 = torch.rand(2, 3)\n","print(random2)\n","\n","torch.manual_seed(1729)\n","random3 = torch.rand(2, 3)\n","print(random3)\n","\n","random4 = torch.rand(2, 3)\n","print(random4)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D83m2jykNUSL","executionInfo":{"status":"ok","timestamp":1710051942926,"user_tz":-540,"elapsed":14,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"058402c2-4aaa-44e5-9979-8a03a4be214c"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n","tensor([[0.2332, 0.4047, 0.2162],\n","        [0.9927, 0.4128, 0.5938]])\n","tensor([[0.3126, 0.3791, 0.3087],\n","        [0.0736, 0.4216, 0.0691]])\n","tensor([[0.2332, 0.4047, 0.2162],\n","        [0.9927, 0.4128, 0.5938]])\n"]}]},{"cell_type":"markdown","source":["* random1과 random3 그리고 random2과 random4, 이 각각 서로 동일한 결과가 나온다는 것을 볼 수 있음.\n","* 무작위 난수 생성기의 seed를 수동으로 설정하면 난수가 재 설정되어 대부분의 환경에서 무작위 숫자에 의존하는 동일한 계산이 이루어지고 동일한 결과를 제공함."],"metadata":{"id":"2ChOiJsJNhpH"}},{"cell_type":"markdown","source":["### Tensor의 shape\n","* 두 개 혹은 그 이상의 tensor에 대한 연산을 수행할 때, tensor들은 같은 shape를 필요로 함. (다시 말해서 차원의 개수가 같아야 하고, 각 차원마다 원소의 수가 같아야 함. 그러기 위해서는 torch.*_like() 함수를 사용함.)"],"metadata":{"id":"3s395JqMNxKD"}},{"cell_type":"code","source":["x = torch.empty(2, 2, 3)\n","print(x.shape)\n","print(x)\n","\n","empty_like_x = torch.empty_like(x)\n","print(empty_like_x.shape)\n","print(empty_like_x)\n","\n","zeros_like_x = torch.zeros_like(x)\n","print(zeros_like_x.shape)\n","print(zeros_like_x)\n","\n","ones_like_x = torch.ones_like(x)\n","print(ones_like_x.shape)\n","print(ones_like_x)\n","\n","rand_like_x = torch.rand_like(x)\n","print(rand_like_x.shape)\n","print(rand_like_x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2Jym8o4tN-LI","executionInfo":{"status":"ok","timestamp":1710051942926,"user_tz":-540,"elapsed":13,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"6bdb8eca-0a00-41ee-d436-95b17a11dc14"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([2, 2, 3])\n","tensor([[[ 6.8669e-08,  3.2356e-41,  6.8810e-08],\n","         [ 3.2356e-41,  8.9683e-44,  0.0000e+00]],\n","\n","        [[ 1.5695e-43,  0.0000e+00, -4.3616e-29],\n","         [ 3.2363e-41,  0.0000e+00,  0.0000e+00]]])\n","torch.Size([2, 2, 3])\n","tensor([[[6.8809e-08, 3.2356e-41, 6.8843e-08],\n","         [3.2356e-41, 1.5456e-42, 0.0000e+00]],\n","\n","        [[6.8846e-08, 3.2356e-41, 1.5414e-44],\n","         [0.0000e+00, 6.8848e-08, 3.2356e-41]]])\n","torch.Size([2, 2, 3])\n","tensor([[[0., 0., 0.],\n","         [0., 0., 0.]],\n","\n","        [[0., 0., 0.],\n","         [0., 0., 0.]]])\n","torch.Size([2, 2, 3])\n","tensor([[[1., 1., 1.],\n","         [1., 1., 1.]],\n","\n","        [[1., 1., 1.],\n","         [1., 1., 1.]]])\n","torch.Size([2, 2, 3])\n","tensor([[[0.6128, 0.1519, 0.0453],\n","         [0.5035, 0.9978, 0.3884]],\n","\n","        [[0.6929, 0.1703, 0.1384],\n","         [0.4759, 0.7481, 0.0361]]])\n"]}]},{"cell_type":"markdown","source":["* 위쪽의 코드 셀에 있는 것들 중에 첫 번째는 tensor에 있는 .shape 속성을 사용함.\n","* 이 속성은 tensor의 각 차원 크기에 대한 리스트를 포함함.\n","* 이 경우에, x는 shape가 2 x 2 x 3 인 3차원 tensor임."],"metadata":{"id":"jsGShd4gOXOT"}},{"cell_type":"markdown","source":["* 그 아래에는, .empty_like(), .zeros_like(), .ones_like(), and .rand_like() 메소드를 호출함.\n","* .shape 속성을 통해서, 위의 메소드들이 동일한 차원값을 반환한다는 것을 검증할 수 있음.\n","* 여기서 다루는 tensor를 생성하는 마지막 방법은 PyTorch collection 형식의 데이터를 직접적으로 명시하는 것."],"metadata":{"id":"oKAucxYKOtJj"}},{"cell_type":"code","source":["some_constants = torch.tensor([[3.1415926, 2.71828], [1.61803, 0.0072897]])\n","print(some_constants)\n","\n","some_integers = torch.tensor((2, 3, 5, 7, 11, 13, 17, 19))\n","print(some_integers)\n","\n","more_integers = torch.tensor(((2, 4, 6), [3, 6, 9]))\n","print(more_integers)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JKBu0Yn6O_7k","executionInfo":{"status":"ok","timestamp":1710051942927,"user_tz":-540,"elapsed":13,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"7929bce6-0615-482d-d5c6-40caa19fa31a"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[3.1416, 2.7183],\n","        [1.6180, 0.0073]])\n","tensor([ 2,  3,  5,  7, 11, 13, 17, 19])\n","tensor([[2, 4, 6],\n","        [3, 6, 9]])\n"]}]},{"cell_type":"markdown","source":["* torch.tensor() 는 이미 Python tuple이나 list 형태로 이루어진 데이터를 가지고 있는 경우 tensor를 만들기 가장 쉬운 방법임.\n","* 위에서 본 것 처럼 중첩된 형태의 collection 자료형은 다차원 tensor가 결과로 나옴."],"metadata":{"id":"Xz8OzuJlPFeP"}},{"cell_type":"markdown","source":["* torch.tensor() 는 데이터의 복사본을 생성함."],"metadata":{"id":"OXBOuIv7PDcX"}},{"cell_type":"markdown","source":["### Tensor 자료형\n","* tensor의 자료형을 설정하는 것은 다양한 방식이 가능함."],"metadata":{"id":"sAYA-OBWPWCi"}},{"cell_type":"code","source":["a = torch.ones((2, 3), dtype=torch.int16)\n","print(a)\n","\n","b = torch.rand((2, 3), dtype=torch.float64) * 20.\n","print(b)\n","\n","c = b.to(torch.int32)\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hN20bzDePZf3","executionInfo":{"status":"ok","timestamp":1710051942927,"user_tz":-540,"elapsed":12,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"81c204f6-f528-4ea8-ace4-355db8ad65c7"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1, 1, 1],\n","        [1, 1, 1]], dtype=torch.int16)\n","tensor([[ 0.9956,  1.4148,  5.8364],\n","        [11.2406, 11.2083, 11.6692]], dtype=torch.float64)\n","tensor([[ 0,  1,  5],\n","        [11, 11, 11]], dtype=torch.int32)\n"]}]},{"cell_type":"markdown","source":["* tensor의 자료형을 설정하는 가장 단순한 방식은 생성할 때 선택적 인자를 사용하는 것.\n","* 위에 있는 cell의 첫 번째 줄을 보면, tensor a를 dtype=torch.int16 자료형으로 설정했음.\n","* a를 출력할 때, 1. 대신에 1로 가득 찬 모습을 볼 수 있음. (파이썬에서 아래 점이 없으면 실수 자료형이 아닌 정수 자료형을 의미함.)"],"metadata":{"id":"7YFnNdlyPjfH"}},{"cell_type":"markdown","source":["* a 를 출력할 때 또 한가지 주목할 점은, dtype을 기본값 (32-bit 부동 소수점) 으로 남길 때와 다르게 tensor를 출력하는 경우 각 tensor의 dtype을 명시한다는 것."],"metadata":{"id":"tNO9gDibP1qY"}},{"cell_type":"markdown","source":["* tensor의 shape를 정수형 인자의 나열, 즉 이 인자를 tuple 자료형 형태로 묶는다는 것을 발견할 수 있음.\n","* 이것은 반드시 필요한 것은 아님. (PyTorch에서는 첫 번째 인자로 tensor shape라는 값을 의미하는 라벨이 없는 정수 인자를 여러개를 받음, 하지만 선택 인자를 추가했을 때, 이 방식은 코드를 더 읽기 쉽게 만들 수 있음.)"],"metadata":{"id":"IJaDdB21QD5k"}},{"cell_type":"markdown","source":["* 자료형을 설정하는 다른 방법은 .to() 메소드랑 함께 사용하는 것.\n","* 위쪽 셀에서 평범한 방식으로 무작위 실수 자료형 tensor b를 생성함.\n","* 이어서 .to() 메소드를 사용해서 b를 32-bit 정수 자료형으로 변환한 c를 생성함.\n","* c는 모든 b의 값과 같은 값을 가지고 있지만 소수점 아래 자리를 버린다는 점이 다름."],"metadata":{"id":"OiOszE0pQScm"}},{"cell_type":"markdown","source":["* 가능한 데이터 자료형은 다음을 포함함.\n","  - torch.bool\n","  - torch.int8\n","  - torch.uint8\n","  - torch.int16\n","  - torch.int32\n","  - torch.int64\n","  - torch.half\n","  - torch.float\n","  - torch.double\n","  - torch.bfloat"],"metadata":{"id":"EOWU9jSHRPhb"}},{"cell_type":"markdown","source":["### PyTorch Tensor에서 산술 & 논리 연산\n","* 지금까지 tensor를 생성하는 몇 가지 방식을 알아봄.\n","* 먼저 기본적인 산술 연산을 알아보고, 그 다음 tensor가 단순 스칼라 값과 어떻게 상호작용 하는지 알아봄."],"metadata":{"id":"csZ6Ky1KRDOd"}},{"cell_type":"code","source":["ones = torch.zeros(2, 2) + 1\n","twos = torch.ones(2, 2) * 2\n","threes = (torch.ones(2, 2) * 7 - 1) / 2\n","fours = twos ** 2\n","sqrt2s = twos ** 0.5\n","\n","print(ones)\n","print(twos)\n","print(threes)\n","print(fours)\n","print(sqrt2s)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKLMVMx6RkdD","executionInfo":{"status":"ok","timestamp":1710051942927,"user_tz":-540,"elapsed":10,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"647113f3-c6a0-4378-f291-30b3c73dcbc1"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[2., 2.],\n","        [2., 2.]])\n","tensor([[3., 3.],\n","        [3., 3.]])\n","tensor([[4., 4.],\n","        [4., 4.]])\n","tensor([[1.4142, 1.4142],\n","        [1.4142, 1.4142]])\n"]}]},{"cell_type":"markdown","source":["* 위에서 볼 수 있듯이 tensor들과 스칼라 값 사이 산술연산, 예를 들면 덧셈, 뺄셈, 곱셈, 나눗셈 그리고 거듭제곱은 tensor의 각 원소에 나눠서 계산을 함.\n","* 이러한 연산의 결과는 tensor가 될 것이기에, threes 변수를 생성하는 줄에서 처럼 일반적인 연산자 우선순위 규칙과 함께 연산자를 연결할 수 있음."],"metadata":{"id":"X6hApFszR3lr"}},{"cell_type":"markdown","source":["* 두  tensor 사이 유사한 연산도 직관적으로 예상할 수 있는 방식으로 동작."],"metadata":{"id":"mbvo3NH8SHjJ"}},{"cell_type":"code","source":["powers2 = twos ** torch.tensor([[1, 2], [3, 4]])\n","print(powers2)\n","\n","fives = ones + fours\n","print(fives)\n","\n","dozens = threes * fours\n","print(dozens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjKA-QuuR1EU","executionInfo":{"status":"ok","timestamp":1710051942927,"user_tz":-540,"elapsed":9,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"9b57d40e-3ee5-44bb-94bc-95fe3a057a20"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 2.,  4.],\n","        [ 8., 16.]])\n","tensor([[5., 5.],\n","        [5., 5.]])\n","tensor([[12., 12.],\n","        [12., 12.]])\n"]}]},{"cell_type":"markdown","source":["* 여기서 주목해야 할 점은 이전 코드 cell에 있는 모든 tensor는 동일한 shape를 가져야 한다는 것.\n","* 만약 서로 다른 shape를 가진 tensor끼리 이진 연산을 수행한다면?"],"metadata":{"id":"41RzvLTjSU7r"}},{"cell_type":"markdown","source":["* 다음 cell은 run-time error가 발생함.\n","* 이것은 의도한 것."],"metadata":{"id":"bUFxEudISd9s"}},{"cell_type":"code","source":["a = torch.rand(2, 3)\n","b = torch.rand(3, 2)\n","\n","print(a * b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":184},"id":"tu7OF_-mSiAl","executionInfo":{"status":"error","timestamp":1710051942927,"user_tz":-540,"elapsed":8,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"1a4a3142-fe2d-42b3-cb20-3b4f764273bc"},"execution_count":10,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-fcc83145fe91>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 1"]}]},{"cell_type":"markdown","source":["* 일반적인 경우에, 다른 shape의 tensor를 이러한 방식으로 연산할 수 없음.\n","* 심지어 위에 있는 cell에 있는 경우처럼 tensor가 서로 같은 개수의 원소를 가지고 있는 경우에도 연산을 할 수 없음."],"metadata":{"id":"xFIlO8UKSlkz"}},{"cell_type":"markdown","source":["### 개요 : Tensor Broadcasting"],"metadata":{"id":"-A3rmNTXStT8"}},{"cell_type":"markdown","source":["* 만약 Numpy의 ndarrays에서 사용하는 broadcasting 문법에 익숙하다면, 여기서도 같은 규칙이 적용된다는 것을 확인할 수 있음."],"metadata":{"id":"cI2g6SzxSxLM"}},{"cell_type":"markdown","source":["* tensor는 같은 shape끼리만 연산이 가능하다는 규칙의 예외가 바로 tensor broadcasting 임."],"metadata":{"id":"s7OA-uCjS2oX"}},{"cell_type":"code","source":["rand = torch.rand(2, 4)\n","doubled = rand * (torch.ones(1, 4) * 2)\n","\n","print(rand)\n","print(doubled)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIDg312_S9V3","executionInfo":{"status":"ok","timestamp":1710051950079,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"415b64b5-4bea-4d71-90e7-85ac5ecf3926"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.2024, 0.5731, 0.7191, 0.4067],\n","        [0.7301, 0.6276, 0.7357, 0.0381]])\n","tensor([[0.4049, 1.1461, 1.4382, 0.8134],\n","        [1.4602, 1.2551, 1.4715, 0.0762]])\n"]}]},{"cell_type":"markdown","source":["* 여기서 사용된 트릭은 무엇일까? (2x4 tensor에 1x4 tensor를 곱한 값?)\n","* Broadcasting은 서로 비슷한 shape를 가진 tensor 사이 연산을 수행하는 방법.\n","* 위의 예시를 보면, 행의 값이 1이고, 열의 값이 4인 tensor가 행의 값이 2이고, 열의 값이 4인 tensor의 모든 행에 곱하게 됨."],"metadata":{"id":"QNtIegSYTIdJ"}},{"cell_type":"markdown","source":["* 이것은 딥러닝에서 중요한 연산.\n","* 일반적인 예시는 학습 가중치 tensor에 입력 tensor의 배치를 곱하고, 배치의 각 인스턴스에 곱하기 연산을 개별적으로 적용한 이후 위의 (2, 4) * (1, 4) tensor연산의 결과가 (2, 4) shape tensor인 것처럼 동일한 shape의 학습 가중치 tensor를 반환하는 것."],"metadata":{"id":"w0FTxH1jTC9E"}},{"cell_type":"markdown","source":["* Broadcasting의 규칙은 다음과 같음.\n","  - 각 tensor는 최소한 1차원 이상을 반드시 가지고 있어야 함. (빈 tensor는 사용할 수 없음.)\n","  - 두 tenor의 각 차원 크기 원소가 다음 조건을 만족하는지 확인하며 비교함. (이때 비교 순서는 맨 뒤에서부터 맨 앞으로 임.)\n","    + 각 차원이 서로 동일함. 또는,\n","    + 각 차원중의 하나의 크기가 반드시 1임. 또는,\n","    + tensor들 중 하나의 차원이 존재하지 않음."],"metadata":{"id":"La6-PdorTnmY"}},{"cell_type":"markdown","source":["* 이전에 봤던 것처럼, 물론 동일한 shape를 가진 Tensor들은 자명하게 \"broadcastable\" 함.\n","* 다음 예제는 위의 규칙을 준수하고 broadcasting을 허용하는 몇 가지 상황임."],"metadata":{"id":"4uGunxqmTqUp"}},{"cell_type":"code","source":["a = torch.ones(4, 3, 2)\n","\n","b = a * torch.rand( 3, 2) # 세 번째와 두 번째 차원이 a랑 동일하고, 첫 번째 차원은 존재하지 않음.\n","print(b)\n","\n","c = a * torch.rand( 3, 1) # 세 번재 차원 = 1이고, 두 번째 차원은 a랑 동일함.\n","print(c)\n","\n","d = a * torch.rand ( 1, 2) # 세 번재 차원이 a와 동일하고, 두 번째 차원 = 1임.\n","print(d)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1wQWTeTmUcVL","executionInfo":{"status":"ok","timestamp":1710051950429,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"56539693-9900-4421-9560-33e103dfe8c7"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.2138, 0.5395],\n","         [0.3686, 0.4007],\n","         [0.7220, 0.8217]],\n","\n","        [[0.2138, 0.5395],\n","         [0.3686, 0.4007],\n","         [0.7220, 0.8217]],\n","\n","        [[0.2138, 0.5395],\n","         [0.3686, 0.4007],\n","         [0.7220, 0.8217]],\n","\n","        [[0.2138, 0.5395],\n","         [0.3686, 0.4007],\n","         [0.7220, 0.8217]]])\n","tensor([[[0.2612, 0.2612],\n","         [0.7375, 0.7375],\n","         [0.8328, 0.8328]],\n","\n","        [[0.2612, 0.2612],\n","         [0.7375, 0.7375],\n","         [0.8328, 0.8328]],\n","\n","        [[0.2612, 0.2612],\n","         [0.7375, 0.7375],\n","         [0.8328, 0.8328]],\n","\n","        [[0.2612, 0.2612],\n","         [0.7375, 0.7375],\n","         [0.8328, 0.8328]]])\n","tensor([[[0.8444, 0.2941],\n","         [0.8444, 0.2941],\n","         [0.8444, 0.2941]],\n","\n","        [[0.8444, 0.2941],\n","         [0.8444, 0.2941],\n","         [0.8444, 0.2941]],\n","\n","        [[0.8444, 0.2941],\n","         [0.8444, 0.2941],\n","         [0.8444, 0.2941]],\n","\n","        [[0.8444, 0.2941],\n","         [0.8444, 0.2941],\n","         [0.8444, 0.2941]]])\n"]}]},{"cell_type":"markdown","source":["* 위의 예시에 있는 각 tensor의 값을 자세히 살펴봄.\n","  - b를 만드는 곱셈 연산은 a의 모든 \"계층\"에 broadcast 됨.\n","  - c에 대해서, 연산은 a의 모든 계층과 행에 대해서 broadcast됨. (모든 열은 3개의 원소값 모두 동일함.\n","  - d에 대해서, 연산이 이전과 반대로 모든 계층과 열에 대해서 수행함. (이제 모든 행이 동일함.)"],"metadata":{"id":"zk1_MrdeWqra"}},{"cell_type":"markdown","source":["* 다음 예시는 broadcasting 시도가 실패한 사례임."],"metadata":{"id":"cL8aKx7sVkSM"}},{"cell_type":"markdown","source":["* 다음 cell은 run-time error 가 발생함. 이것은 의도한 것."],"metadata":{"id":"p6voEWlWW8Cm"}},{"cell_type":"code","source":["a = torch.ones(4, 3, 2)\n","b = a * torch.rand(4, 3) # 차원은 반드시 맨 뒤 원소부터 맨 앞 원소로 차례대로 맞춰야 함.\n","c = a * torch.rand( 2, 3) # 세번째와 두 번째 차원 모두 서로 다름.\n","d = a * torch.rand((0, )) # 비어있는 tensor는 broadcast 할 수 없음."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"ea2vjPfRXEus","executionInfo":{"status":"error","timestamp":1710051956324,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"6ac1b906-919d-4367-de3f-c353b3ccd302"},"execution_count":13,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-13-8572eedeb94b>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 차원은 반드시 맨 뒤 원소부터 맨 앞 원소로 차례대로 맞춰야 함.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 세번째와 두 번째 차원 모두 서로 다름.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 비어있는 tensor는 broadcast 할 수 없음.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (3) at non-singleton dimension 2"]}]},{"cell_type":"markdown","source":["### Tensor를 사용하는 다양한 연산들\n","* PyTorch tensor는 tensor들끼리 수행할 수 있는 300개 이상의 연산을 가지고 있음.\n","* 다음 작은 예시는 주로 사용하는 연산 종류 몇 개를 보여줌."],"metadata":{"id":"GAflKbefXThH"}},{"cell_type":"code","source":["# 공용 함수\n","a = torch.rand(2, 4) * 2 - 1\n","print('Common functions:')\n","print(torch.abs(a))\n","print(torch.ceil(a))\n","print(torch.floor(a))\n","print(torch.clamp(a, -0.5, 0.5))\n","\n","# 삼각 함수와 그 역함수들\n","angles = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","sines = torch.sin(angles)\n","inverses = torch.asin(sines)\n","print('\\nSine and arcsine:')\n","print(angles)\n","print(sines)\n","print(inverses)\n","\n","# 비트 연산\n","print('\\nBitwise XOR:')\n","b = torch.tensor([1, 5, 11])\n","c = torch.tensor([2, 7, 10])\n","print(torch.bitwise_xor(b, c))\n","\n","# 비교 연산:\n","print('\\nBroadcasted, element-wise equality comparison:')\n","d = torch.tensor([[1., 2.], [3., 4.]])\n","e = torch.ones(1, 2)  # 많은 비교 연산자들은 broadcasting을 지원합니다!\n","print(torch.eq(d, e)) # bool 자료형을 가진 tensor를 반환합니다.\n","\n","# 차원 감소 연산:\n","print('\\nReduction ops:')\n","print(torch.max(d))        # 단일 원소 tensor를 반환합니다.\n","print(torch.max(d).item()) # 반환한 tensor로부터 값을 추출합니다.\n","print(torch.mean(d))       # 평균\n","print(torch.std(d))        # 표준 편차\n","print(torch.prod(d))       # 모든 숫자의 곱\n","print(torch.unique(torch.tensor([1, 2, 1, 2, 1, 2]))) # 중복되지 않은 값들을 걸러냅니다.\n","\n","# 벡터와 선형 대수 연산\n","v1 = torch.tensor([1., 0., 0.])         # x축 단위 벡터\n","v2 = torch.tensor([0., 1., 0.])         # y축 단위 벡터\n","m1 = torch.rand(2, 2)                   # 무작위 행렬\n","m2 = torch.tensor([[3., 0.], [0., 3.]]) # 단위 행렬에 3을 곱한 결과\n","\n","print('\\nVectors & Matrices:')\n","print(torch.cross(v2, v1)) # z축 단위 벡터의 음수값 (v1 x v2 == -v2 x v1)\n","print(m1)\n","m3 = torch.matmul(m1, m2)\n","print(m3)                  # m1 행렬을 3번 곱한 결과\n","print(torch.svd(m3))       # 특이값 분해"],"metadata":{"id":"K2SdyafPYjzN","executionInfo":{"status":"aborted","timestamp":1710051956325,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["* 이것은 수많은 연산의 일부분.\n","### Tensor의 값을 변경하기\n","* 대부분 tensor들의 이진 연산은 제 3자의 새로운 tensor를 생성함.\n","* c = a * b (a와 b는 tensor)연산을 수행할 때, 새로운 tensor c는 다른 tensor와 구별되는 메모리 영역을 차지하게 됨."],"metadata":{"id":"5wrnlP7sZQ6v"}},{"cell_type":"markdown","source":["* 그럼에도 불구하고 tensor의 값을 변경하고 싶은 순간이 있을 수가 있음.\n","* 예를 들어, 중간 연산 결과 값을 버릴 수 있는 각 원소 단위 연산을 수행하는 경우가 있음.\n","* 이런 연산을 위해, 대부분의 수학 함수들은 tensor 내부의 값을 변경할 수 있는 함수 이름 맨 뒤에 밑줄 (_)이 추가된 버전을 가지고 있음."],"metadata":{"id":"H2olu2y6bL8l"}},{"cell_type":"code","source":["a = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('a:')\n","print(a)\n","print(torch.sin(a)) # 이 연산은 메모리에 새로운 tensor를 생성함.\n","print(a) # a는 변하지 않음.\n","\n","b = torch.tensor([0, math.pi / 4, math.pi / 2, 3 * math.pi / 4])\n","print('\\nb:')\n","print(b)\n","print(torch.sin_(b)) # 밑줄에 주목하기\n","print(b) # b가 변함."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"P-RxswhXbZ6J","executionInfo":{"status":"ok","timestamp":1710051957052,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"389bf91d-7980-4ae4-9580-26cec750f0ec"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["a:\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","\n","b:\n","tensor([0.0000, 0.7854, 1.5708, 2.3562])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n","tensor([0.0000, 0.7071, 1.0000, 0.7071])\n"]}]},{"cell_type":"markdown","source":["* 산술 연산에서, 비슷한 행동을 하는 함수가 있음."],"metadata":{"id":"mkZNHZ7hb9ne"}},{"cell_type":"code","source":["a = torch.ones(2, 2)\n","b = torch.rand(2, 2)\n","\n","print('Before')\n","print(a)\n","print(b)\n","print('\\nAfter adding:')\n","print(a.add_(b))\n","print(a)\n","print(b)\n","print('\\nAfter multiplying')\n","print(b.mul_(b))\n","print(b)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jdUnzxl1bm2b","executionInfo":{"status":"ok","timestamp":1710051957727,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"b013411d-b36a-4a31-e849-c08c4d67000e"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Before\n","tensor([[1., 1.],\n","        [1., 1.]])\n","tensor([[0.0776, 0.4004],\n","        [0.9877, 0.0352]])\n","\n","After adding:\n","tensor([[1.0776, 1.4004],\n","        [1.9877, 1.0352]])\n","tensor([[1.0776, 1.4004],\n","        [1.9877, 1.0352]])\n","tensor([[0.0776, 0.4004],\n","        [0.9877, 0.0352]])\n","\n","After multiplying\n","tensor([[0.0060, 0.1603],\n","        [0.9756, 0.0012]])\n","tensor([[0.0060, 0.1603],\n","        [0.9756, 0.0012]])\n"]}]},{"cell_type":"markdown","source":["* 이러한 내부의 값을 변경하는 산술 함수는 다른 많은 함수들 (e.g : torch.sin())처럼 torch 모듈의 메소드가 아니라 torch.Tensor 객체의 메소드인 점에 주목해야 함.\n","* a.add_(b)와 같은 경우처럼, 메소드를 호출하는 tensor는 값이 변경됨."],"metadata":{"id":"WOZPUyxzc2K1"}},{"cell_type":"markdown","source":["* 이미 존재하고 있는 메모리에 할당된 tensor에 계산 결과값을 저장하는 또 다른 옵션이 있음.\n","* tensor를 생성하는 메소드 뿐만 아니라 지금까지 이 문서에서 봤던 수많은 함수나 메소드는 결과 값을 받는 특정 tensor를 명시하는 out 이라는 인자를 가지고 있음.\n","* 만약 out tensor가 알맞은 shape와 dtype을 가지고 있다면, 새로운 메모리 할당 없이 결과값이 저장됨."],"metadata":{"id":"baCTts7rdHJk"}},{"cell_type":"code","source":["a = torch.rand(2, 2)\n","b = torch.rand(2, 2)\n","c = torch.zeros(2, 2)\n","old_id = id(c)\n","\n","print(c)\n","d = torch.matmul(a, b, out=c)\n","print(c) # c의 값이 변경되었음.\n","\n","assert c is d # c와 d가 서로 단순히 같은 값을 가지는지가 아니라 같은 객체인지 테스트함.\n","assert id(c) == old_id # 새로운 c는 이전 객체와 확실히 같은 객체임.\n","\n","torch.rand(2, 2, out=c) # 다시 한번 생성해봄.\n","print(c) # c의 값이 다시 바뀌었음.\n","assert id(c) == old_id # 하지만 여전히 같은 객체"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MoYZHJx3dVSH","executionInfo":{"status":"ok","timestamp":1710051958054,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"585bc166-8931-4b84-fcf6-ee8c20cb5bb3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0., 0.],\n","        [0., 0.]])\n","tensor([[0.4101, 0.1728],\n","        [0.8007, 0.7183]])\n","tensor([[0.8441, 0.9004],\n","        [0.3995, 0.6324]])\n"]}]},{"cell_type":"markdown","source":["### Tensor를 복사하기\n","* 파이썬의 다른 객체와 마찬가지로 변수에 tensor를 할당하는 것은 변수가 tensor의 label이 되고 값을 복사하지 않음."],"metadata":{"id":"pJdY5EgDd2-X"}},{"cell_type":"code","source":["a = torch.ones(2, 2)\n","b = a\n","\n","a[0][1] = 561 # a의 값을 바꾸면...\n","print(b) # ...b의 값이 바뀜."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ucY0fsU4e9sD","executionInfo":{"status":"ok","timestamp":1710051958319,"user_tz":-540,"elapsed":1,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"066ae55d-1468-4b9a-afd0-398c2ba31dc8"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[  1., 561.],\n","        [  1.,   1.]])\n"]}]},{"cell_type":"markdown","source":["* 하지만 만약 우리가 작업할 별도의 데이터 복사본을 원하면?\n","* clone() 메소드가 해답이 됨."],"metadata":{"id":"IiHhu7ogfE6O"}},{"cell_type":"code","source":["a = torch.ones(2, 2)\n","b = a.clone()\n","\n","assert b is not a # 메모리 상의 다른 객체임...\n","print(torch.eq(a, b)) # ... 하지만 여전히 같은 값을 가지고 있음.\n","\n","a[0][1] = 561 # a가 변경되었음...\n","print(b) # ...하지만 여전히 b는 이전 값을 가지고 있음."],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EFFqSsGxfKsg","executionInfo":{"status":"ok","timestamp":1710051960616,"user_tz":-540,"elapsed":269,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"3cba59df-fcdb-43cd-8251-366b0132efea"},"execution_count":18,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[True, True],\n","        [True, True]])\n","tensor([[1., 1.],\n","        [1., 1.]])\n"]}]},{"cell_type":"markdown","source":["* **\"clone()메소드를 사용할 때 알아야 할 중요한 점이 있음.\"** 만약 source tensor가 autograd를 가진다면 clone이 가능함.\n","* **이 부분은 autograd와 관련된 동영상에서 더 깊이 다룰 것.**\n","* 대부분의 경우에서 이것이 바로 원하던 것.\n","* 예를 들어, 여러분의 모델이 그 모델의 forward() 메소드에 여러 갈래의 계산 경로가 있고 원본 tensor와 그것의 복제본 모두가 모델의 출력에 기여를 한다면, 두 tensor에 대한 autograd를 설정하는 모델 학습을 활성화 함.\n","* 만약 여러분의 source tensor가 autograd를 사용할 수 있다면 (일반적으로 학습 가중치의 집합이거나, 가중치를 포함하는 계산에서 파생된 경우), 여러분이 원하는 결과를 얻을 수 있음."],"metadata":{"id":"zAY99ktAfXWS"}},{"cell_type":"markdown","source":["* 반면에 원본 tensor나 그것의 복제본 모두가 변화도를 추적할 필요가 없다면, source tensor의 autograd가 꺼져있다면 clone을 사용할 수 있음."],"metadata":{"id":"5ZVZ6N5JgGSI"}},{"cell_type":"markdown","source":["* 그러나 세번째 경우가 있음. 기본적으로 변화도가 모든 것을 위해 켜져있지만 일부 지표를 생성하기 위해 스트림 중간에서 일부 값을 생성하고 싶어하는 여러분 모델의 forward() 함수에서 계산을 수행한다고 상상.\n","* 이 경우에는 변화도를 추적하기 위해서 source tensor의 복제본을 원하지 않을 수 있음. (성능이 autograd의 히스토리 추적 기능을 끄면서 향상됨. 이 경우를 위해서는 source tensor에 .detach() 메소드를 사용할 수 있음.)"],"metadata":{"id":"1Fgeg_EFgQyH"}},{"cell_type":"code","source":["a = torch.rand(2, 2, requires_grad=True) # autograd를 켬.\n","print(a)\n","\n","b = a.clone()\n","print(b)\n","\n","c = a.detach().clone()\n","print(c)\n","\n","print(a)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"z2wI8PNTghyj","executionInfo":{"status":"ok","timestamp":1710051962325,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"9ae38d9a-d7b3-424b-e923-9936ac5e1034"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9464, 0.0113],\n","        [0.5183, 0.9807]], requires_grad=True)\n","tensor([[0.9464, 0.0113],\n","        [0.5183, 0.9807]], grad_fn=<CloneBackward0>)\n","tensor([[0.9464, 0.0113],\n","        [0.5183, 0.9807]])\n","tensor([[0.9464, 0.0113],\n","        [0.5183, 0.9807]], requires_grad=True)\n"]}]},{"cell_type":"markdown","source":["* 여기서 일어나는 일.\n","  - a를 requires_grad=True 옵션을 킨 상태로 생성함.\n","  - **아직 이 선택적 인자를 다루지 않았지만, autograd 단원 동안만 다룰 것.**\n","  - a를 출력할 때, requires_grad=True 속성을 가지고 있다고 알려줌. (이 뜻은 autograd와 게산 히스토리 추적 기능을 켠다는 것.)\n","  - a를 복제하고 그것을 b라고 라벨을 붙임.\n","  - b를 출력할 때, 그것의 계산 히스토리가 추적되고 있다는 것을 확인할 수 있음. (a의 autograd 설정에 내장되어 있는 기능이며, 이 설정은 계산 히스토리에 추가함.)\n","  - a를 c에 복제를 하지만 detach()를 먼저 호출함.\n","  - c를 출력함. (계산 히스토리가 없다는 것을 확인할 수 있고, requires_grad=True 옵션이 없다는 것 또한 확인 가능.)"],"metadata":{"id":"6gTDeHojg5y4"}},{"cell_type":"markdown","source":["* detach() 메소드는 tensor의 계산 히스토리로부터 tensor를 떼어냄.\n","* 이 메소드의 의미는 \"메소드 뒤에 어떤 것이든 와도 autograd를 끈 것처럼 작동하라.\"라는 뜻임.\n","* a를 변경하지 않고 이 메소드를 수행함. (마지막에 a를 다시 출력할 때, 여전히 a가 가진 requires_grad=True 속성이 남아 있다는 것을 확인할 수 있음.)"],"metadata":{"id":"8zgeHahLhxRr"}},{"cell_type":"markdown","source":["### GPU 환경으로 이동하기\n","* PyTorch의 주된 장점 중 하나는 CUDA가 호환되는 Nvidia GPU에서의 강력한 성능 가속화. (\"CUDA\"는 Compute Unified Device Architecture의 약자이며, 병렬 컴퓨팅을 위한 Nvidia의 플랫폼임.)\n","* 지금까지 모든 작업을 CPU에서 처리함.\n","* 어떻게 더 빠른 하드웨어로 이동할 수 있을까요?"],"metadata":{"id":"6hhhBBk5iCdK"}},{"cell_type":"markdown","source":["* 먼저 is_available() 메소드를 사용해서 GPU가 사용 가능한지 아닌지 확인해야 함."],"metadata":{"id":"vQw0DUTpiRpX"}},{"cell_type":"markdown","source":["* 만약 CUDA가 호환되는 GPU가 없고 CUDA 드라이버가 설치되어 있지 않다면 이 섹션에서 실행 가능한 cell은 어떤 GPU와 관련된 코드도 실행할 수 없음."],"metadata":{"id":"Nra8IPSEiVLg"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  print('We have a GPU')\n","else:\n","  print('Sorry, CPU only.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nRYCcFD4icLs","executionInfo":{"status":"ok","timestamp":1710051964184,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"36869224-e317-4ff0-d515-c83dade571fd"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["We have a GPU\n"]}]},{"cell_type":"markdown","source":["* 일단 1개 혹은 그 이상의 GPU가 사용 가능하다는 것을 확인했다면, 데이터를 GPU가 확인할 수 있는 어떤 공간에 저장할 필요가 있음.\n","* CPU는 컴퓨터의 RAM에서 데이터를 이용해서 계산을 수행함.\n","* GPU는 전용 메모리가 연결되어 있음.\n","* 해당 장치에서 계산을 수행하고 싶을 때마다 계산에 필요한 모든 데이터를 GPU 장치가 접근 가능한 메모리로 이동해야 함. (평소에는 \"GPU가 접근 가능한 메모리로 데이터를 이동한다\"를 \"데이터를 GPU로 옮긴다\"라고 줄여서 말함.)"],"metadata":{"id":"qwns1UD-ihhy"}},{"cell_type":"markdown","source":["* 목적 장치에서 데이터를 가져오는 다양한 방법이 있음.\n","* 객체를 생성할 때 데이터를 가져올 수 있음."],"metadata":{"id":"gb6-A7XPi0u8"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  gpu_rand = torch.rand(2, 2, device='cuda')\n","  print(gpu_rand)\n","else:\n","  print('Sorry, CPU only.')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DNal0AM_i8Gt","executionInfo":{"status":"ok","timestamp":1710051966027,"user_tz":-540,"elapsed":836,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"162d0071-adc8-47b7-cc1b-fe1e020bd72b"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.3344, 0.2640],\n","        [0.2119, 0.0582]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["* 기본적으로 새로운 tensor는 CPU에 생성됨.\n","* 따라서 tensor를 GPU에 생성하고 싶을 때 device 선택 인자를 반드시 명시해줘야 함.\n","* 새로운 tensor를 출력할 때, (만약 CPU에 존재하지 않는다면) PyTorch는 어느 장치에 객체가 있는지 알려준다는 것을 확인할 수 있음."],"metadata":{"id":"yV1PFCeHjFLh"}},{"cell_type":"markdown","source":["* torch.cuda.device_count() 를 사용해서 GPU의 개수를 조회할 수 있음.\n","* 만약 1개보다 많은 GPU를 가지고 있다면, 각 GPU를 인덱스로 지정할 수 있음. (device='cuda:0', device='cuda:1' 와 같이)"],"metadata":{"id":"1_0WGG5OjDh3"}},{"cell_type":"markdown","source":["* 코딩을 할 때, 어디에서나 장치 이름을 문자열 상수로 지정하는 것은 상당히 유지 보수에 취약함.\n","* CPU 하드웨어나 GPU 하드웨어 어떤 것을 사용하는지에 관계없이 여러분의 코드는 잘 작동해야 함.\n","* 문자열 대신에 tensor를 저장할 장치 핸들러를 생성하는 것으로 유지 보수가 쉬운 코드를 작성할 수 있음."],"metadata":{"id":"vRJhvX8-jh5p"}},{"cell_type":"code","source":["if torch.cuda.is_available():\n","  my_device = torch.device('cuda')\n","else:\n","  my_device = torch.device('cpu')\n","print('Device: {}'.format(my_device))\n","\n","x = torch.rand(2, 2, device=my_device)\n","print(x)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEgUdaZejuUU","executionInfo":{"status":"ok","timestamp":1710051966822,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"37ddc2ee-1228-451f-dba6-4c77e8ca4fd3"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n","tensor([[0.0024, 0.6778],\n","        [0.2441, 0.6812]], device='cuda:0')\n"]}]},{"cell_type":"markdown","source":["* 만약 한 장치에 tensor가 있을 때, to() 메소드를 사용해서 다른 장치로 이동할 수 있음.\n","* 다음 코드는 CPU에 tensor를 생성하고, 이전 cell에서 얻은 장치 핸들러로 tensor를 이동함."],"metadata":{"id":"0QU971s7j8a1"}},{"cell_type":"code","source":["y = torch.rand(2, 2)\n","y = y.to(my_device)"],"metadata":{"id":"LnqA97JnkFD8","executionInfo":{"status":"ok","timestamp":1710051967336,"user_tz":-540,"elapsed":1,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["* 2개 혹은 그 이상의 tensor를 포함한 계산을 하기 위해서는 모든 tensor가 같은 장치에 있어야 한다는 것을 아는 것이 중요함.\n","* 다음 코드는 GPU 장치가 사용 가능 하다는 것과 관계 없이 runtime error를 발생할 것."],"metadata":{"id":"rBg7Va3gkHpY"}},{"cell_type":"code","source":["x = torch.rand(2, 2)\n","y = torch.rand(2, 2, device='gpu')\n","z = x + y # 오류가 발생할 것."],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":202},"id":"wqg7DxSekRpr","executionInfo":{"status":"error","timestamp":1710051968387,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"666e09f5-ba52-47b4-ba7d-abfd862c83c4"},"execution_count":24,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-ba6acd75e98e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'gpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;31m# 오류가 발생할 것.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Expected one of cpu, cuda, ipu, xpu, mkldnn, opengl, opencl, ideep, hip, ve, fpga, ort, xla, lazy, vulkan, mps, meta, hpu, mtia, privateuseone device type at start of device string: gpu"]}]},{"cell_type":"markdown","source":["### Tensor의 shape 다루기\n","* 때로는 tensor의 shape를 변환할 필요가 있음.\n","* 아래에 있는 몇 가지 흔한 경우와 함께 tensor의 shape를 다루는 방법에 대해 알아볼 것."],"metadata":{"id":"CFkS382nkZBv"}},{"cell_type":"markdown","source":["### 차원의 개수 변경하기\n","* 차원의 개수를 변경할 필요가 있는 한가지 경우는 모델의 입력에 단일 인스턴스를 전달할 때.\n","* PyTorch 모델은 일반적으로 입력에 배치가 들어오기를 기대함."],"metadata":{"id":"tg-gI7jbkgJt"}},{"cell_type":"markdown","source":["* 예를 들어, 3개의 색깔 채널을 가진 226 픽셀 정사각형 이미지인 3 x 226 x 226 개 데이터와 함께 작동하는 모델을 가지고 있다고 상상.\n","* 이미지를 불러오고 tensor로 변환하면 (3, 226, 226) shape를 가진 tensor가 됨.\n","* 그럼에도 불구하고 이 모델을 (N, 3, 226, 226) shape를 가진 tensor를 입력으로 기대함.\n","* 이때 N은 배치에 포함된 이미지의 개수.\n","* 그렇다면 어떻게 한 배치를 만들 수 있을까요?"],"metadata":{"id":"43fBbKQNknkK"}},{"cell_type":"code","source":["a = torch.rand(3, 226, 226)\n","b = a.unsqueeze(0)\n","\n","print(a.shape)\n","print(b.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yoEkSMFCk5uK","executionInfo":{"status":"ok","timestamp":1710051970072,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"6aec6950-7581-4ed4-8468-a07b9b8300df"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 226, 226])\n","torch.Size([1, 3, 226, 226])\n"]}]},{"cell_type":"markdown","source":["* unsqueeze() 메소드는 크기가 1인 차원을 추가함.\n","* unsqueeze(0) 는 새로운 0번째 차원을 추가함. (이제 한 배치를 가지게 되었음.)"],"metadata":{"id":"puIc_wJ9lA7A"}},{"cell_type":"markdown","source":["* 이게 unsqueezing이면, squeezing은 무슨 뜻?\n","* 여기서는 차원을 하나 확장해도 tensor에 있는 원소의 개수는 변하지 않는다는 사실을 이용하고 있음."],"metadata":{"id":"_6lkT0rOlIlD"}},{"cell_type":"code","source":["c = torch.rand(1, 1, 1, 1, 1)\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"293ZBA_nlOwn","executionInfo":{"status":"ok","timestamp":1710051970805,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"e53c023c-47f9-405e-f168-cf18996ea91e"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[[0.5057]]]]])\n"]}]},{"cell_type":"markdown","source":["* 위의 예제에 이어서 각 입력 값에 대한 모델의 출력 값이 20개의 원소를 가진 vector라고 생각.\n","* 그렇다면, N이 입력 배치에 있는 인스턴스의 개수라고 할 때, 출력 값의 shape는 (N, 20)라고 기대할 수 있음.\n","* 이 뜻은 입력으로 단일 배치가 들어왔을 때, (1, 20)의 shape를 가진 출력 값을 얻는 다는 것."],"metadata":{"id":"HT2m0CfjlSOq"}},{"cell_type":"markdown","source":["* 만약 그저 20개의 원소를 가진 벡터와 같이 배치가 shape가 아닌 연산 결과를 얻고 싶다면?"],"metadata":{"id":"F2m0AICbleBG"}},{"cell_type":"code","source":["a = torch.rand(1, 20)\n","print(a.shape)\n","print(a)\n","\n","b = a.squeeze(0)\n","print(b.shape)\n","print(b)\n","\n","c = torch.rand(2, 2)\n","print(c.shape)\n","\n","d = c.squeeze(0)\n","print(d.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RAQJU5kKlkYA","executionInfo":{"status":"ok","timestamp":1710051971608,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"273ba5ca-2bda-4483-a847-c91185c0b8f9"},"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([1, 20])\n","tensor([[0.9335, 0.9769, 0.2792, 0.3277, 0.5210, 0.7349, 0.7823, 0.8637, 0.1891,\n","         0.3952, 0.9176, 0.8960, 0.4887, 0.8625, 0.6191, 0.9935, 0.1844, 0.6138,\n","         0.6854, 0.0438]])\n","torch.Size([20])\n","tensor([0.9335, 0.9769, 0.2792, 0.3277, 0.5210, 0.7349, 0.7823, 0.8637, 0.1891,\n","        0.3952, 0.9176, 0.8960, 0.4887, 0.8625, 0.6191, 0.9935, 0.1844, 0.6138,\n","        0.6854, 0.0438])\n","torch.Size([2, 2])\n","torch.Size([2, 2])\n"]}]},{"cell_type":"markdown","source":["* 결과로 나온 shape로 부터 2차원 tensor가 이제 1차원으로 바뀐 것을 볼 수 있고, 위에 있는 cell의 결과를 자세히 보면 추가적인 차원을 가졌기 때문에 a를 출력하는 것에서 \"추가\" 대괄호 집합 []을 볼 수 있음."],"metadata":{"id":"16p5PJmVl5ca"}},{"cell_type":"markdown","source":["* 오직 차원의 값이 1인 경우에만 squeeze()를 사용할 수 있음.\n","* c에서 크기가 2인 차원을 squeeze 하려고 하는 위 예시를 보면, 처음 그 shape로 다시 돌아온다는 사실을 알 수 있음.\n","* squeeze() 와 unsqueeze() 를 호출하는 것은 오직 차원의 크기가 1일 때만 작동함.\n","* 왜냐하면 이 경우가 아니면 tensor의 원소 개수가 바뀌기 때문임.\n","* unsqueeze() 는 broadcasting을 쉽게 하는 경우에도 사용함."],"metadata":{"id":"-sA_ocpBlv1J"}},{"cell_type":"code","source":["a = torch.ones(4, 3, 2)\n","\n","c = a * torch.rand( 3, 1) # 3번째 차원 = 1, 2번째 차원은 다음 코드랑 동일함.\n","print(c)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jy2gqs1em-xi","executionInfo":{"status":"ok","timestamp":1710051972514,"user_tz":-540,"elapsed":1,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"c739ef3d-e8d1-4eb6-fc6c-8c2b330da01b"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[0.1394, 0.1394],\n","         [0.1721, 0.1721],\n","         [0.1751, 0.1751]],\n","\n","        [[0.1394, 0.1394],\n","         [0.1721, 0.1721],\n","         [0.1751, 0.1751]],\n","\n","        [[0.1394, 0.1394],\n","         [0.1721, 0.1721],\n","         [0.1751, 0.1751]],\n","\n","        [[0.1394, 0.1394],\n","         [0.1721, 0.1721],\n","         [0.1751, 0.1751]]])\n"]}]},{"cell_type":"markdown","source":["* broadcast의 순수한 효과는 차원 0과 차원 2에 대한 연산을 broadcast해서 무작위 3 x 1 shape의 tensor를 a의 원소 개수가 3인 모든 열에 곱하는 것이었음."],"metadata":{"id":"SqiMdOv6n9zq"}},{"cell_type":"markdown","source":["* 만약 무작위 벡터가 오직 3개의 원소만을 가지면 어떻게 될까요?\n","* broadcast를 할 능력을 잃어버리게 됨.\n","* 왜냐하면 마지막 차원이 broadcasting 규칙에 맞지 않기 때문임.\n","* 하지만 unsqueeze() 가 도와줌."],"metadata":{"id":"FcQdburaDOyJ"}},{"cell_type":"code","source":["a = torch.ones(4, 3, 2)\n","b = torch.rand( 3) # a * b 를 시도하는 것은 runtime error가 발생함.\n","c = b.unsqueeze(1) # 끝에 새로운 차원을 추가해서 2차원 tensor로 바꿈.\n","print(c.shape)\n","print(a * c) # broadcasting 이 다시 작동함!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vibgiEVaDYxZ","executionInfo":{"status":"ok","timestamp":1710052230599,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"6201082a-cca5-4ab4-e104-fd9179cd4799"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([3, 1])\n","tensor([[[0.3851, 0.3851],\n","         [0.0732, 0.0732],\n","         [0.3118, 0.3118]],\n","\n","        [[0.3851, 0.3851],\n","         [0.0732, 0.0732],\n","         [0.3118, 0.3118]],\n","\n","        [[0.3851, 0.3851],\n","         [0.0732, 0.0732],\n","         [0.3118, 0.3118]],\n","\n","        [[0.3851, 0.3851],\n","         [0.0732, 0.0732],\n","         [0.3118, 0.3118]]])\n"]}]},{"cell_type":"markdown","source":["* 때로는 원소의 개수와 원소의 값을 여전히 유지하면서 tensor의 shape를 한번에 바꾸고 싶을 때가 있음.\n","* 모델의 합성곱 계층과 선형 계층 사이 인터페이스에서 이러한 상황이 발생함. (이 상황은 이미지 분류 모델에서 흔히 일어나는 일.)\n","* 합성곱 커널은 특성의 수 x 너비 x 높이 shape의 tensor를 출력 값으로 생성하지만 이후에 있는 선형 게층은 입력 값으로 1차원을 기대함.\n","* 여러분이 요청한 차원에 입력 tensor가 가진 원소와 같은 개수를 생성하는 reshape() 를 여러분을 위해서 제공함."],"metadata":{"id":"Fbg_r4yKDkMk"}},{"cell_type":"code","source":["output3d = torch.rand(6, 20, 20)\n","print(output3d.shape)\n","\n","input1d = output3d.reshape(6 * 20 * 20)\n","print(input1d.shape)\n","\n","# torch 모듈에 있는 메소드에 대해서도 호출할 수 있음.\n","print(torch.reshape(output3d, (6 * 20 * 20,)).shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVW6gWRZD6pv","executionInfo":{"status":"ok","timestamp":1710052379493,"user_tz":-540,"elapsed":244,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"6940ac84-0ccd-4a30-8ead-4f5b7d408477"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([6, 20, 20])\n","torch.Size([2400])\n","torch.Size([2400])\n"]}]},{"cell_type":"markdown","source":["* 위에 있는 cell의 마지막 줄에 있는 인자 (6 * 20 * 20) 는 PyTorch는 tensor shape를 나타낼 때 **tuple**을 기대하기 때문.\n","* 하지만, shape가 메소드의 첫번째 인자라면, 연속된 정수라고 속여서 사용할 수 있음.\n","* 여기에서는 메소드에게 이 인자가 진짜 1개 원소를 가진 튜플이라고 알려주기 위해서 편의상 소괄호와 콤마를 추가해야 함."],"metadata":{"id":"d8l_tB4HEIfm"}},{"cell_type":"markdown","source":["* reshape() 는 tensor를 바라보는 관점을 변경함. (즉, 메모리의 같은 지역을 바라보는 서로 다른 관점을 가진 tensor 객체라는 뜻.)\n","* source tensor에 어떠한 변화가 있으면 clone() 을 사용하지 않는 한, 해당 tensor를 바라보고 있는 다른 객체 또한 값이 변한다는 뜻."],"metadata":{"id":"dK0n5YSXEZjD"}},{"cell_type":"markdown","source":["* 해당 소개의 범위를 벗어난 조건들이 있음.\n","* 그것은 reshape() 가 data의 복사본을 가진 tensor를 반환 해야 한다는 것."],"metadata":{"id":"nunlmxCBFAXd"}},{"cell_type":"markdown","source":["### NumPy로 변환"],"metadata":{"id":"WYdzQYPTFFik"}},{"cell_type":"markdown","source":["* 위에 있는 broadcasting 부분에서, PyTorch의 broadcast 문법은 Numpy와 호환 가능하다고 말함. (하지만, PyTorch와 NumPy 사이 유사성은 우리가 생각한 것 보다 더욱 깊음.)"],"metadata":{"id":"zZ54o3lIFHcN"}},{"cell_type":"markdown","source":["* 만약 NumPy의 ndarrays에 저장되어 있는 데이터를 사용하는 머신 러닝 혹은 과학 분야와 관련된 코드를 가지고 있다면, 같은 데이터를 PyTorch의 GPU 가속을 사용할 수 있고 머신 러닝 모델을 만드는 데 필요한 효과적인 추상화를 제공하는 PyTorch tensor로 표현하고 싶을 수 있음.\n","* ndarray와 PyTorch tensor끼리 바꾸는 것은 쉬움."],"metadata":{"id":"8NhYE-4sFO65"}},{"cell_type":"code","source":["import numpy as np\n","\n","numpy_array = np.ones((2, 3))\n","print(numpy_array)\n","\n","pytorch_tensor = torch.from_numpy(numpy_array)\n","print(pytorch_tensor)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_1eLTW8eFcgx","executionInfo":{"status":"ok","timestamp":1710052757847,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"66e72b1f-40b7-4ade-c077-ea537863ad07"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1. 1. 1.]\n"," [1. 1. 1.]]\n","tensor([[1., 1., 1.],\n","        [1., 1., 1.]], dtype=torch.float64)\n"]}]},{"cell_type":"markdown","source":["* PyTorch는 NumPy array와 같은 shape의 tensor를 생성하고, 같은 데이터를 포함.\n","* 심지어 NumPy의 기본적인 64비트 실수 데이터 자료형을 유지함."],"metadata":{"id":"T6qD2a3zFk7V"}},{"cell_type":"markdown","source":["* PyTorch에서 NumPy로 변환은 다른 방식을 사용해서 쉽게 할 수 있음."],"metadata":{"id":"I6zdo-wKFsjg"}},{"cell_type":"code","source":["pytorch_rand = torch.rand(2, 3)\n","print(pytorch_rand)\n","\n","numpy_rand = pytorch_rand.numpy()\n","print(numpy_rand)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G1KLh_E4FvU_","executionInfo":{"status":"ok","timestamp":1710052821638,"user_tz":-540,"elapsed":3,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"57609fbd-1768-45bf-ee19-654728f3a627"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[0.9509, 0.9587, 0.4899],\n","        [0.8069, 0.8094, 0.1614]])\n","[[0.9508812 0.9586813 0.4898774]\n"," [0.8068895 0.8093723 0.1613965]]\n"]}]},{"cell_type":"markdown","source":["* 이러한 변환된 객체들은 해당 객체의 source 객체가 위치한 메모리의 같은 공간을 사용한다는 점을 아는 것이 중요함.\n","* 이것은 한 객체가 변하면 다른 것에 영향을 준다는 의미."],"metadata":{"id":"-72dhROoF0eH"}},{"cell_type":"code","source":["numpy_array[1, 1] = 23\n","print(pytorch_tensor)\n","\n","pytorch_rand[1, 1] = 17\n","print(numpy_rand)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8LEAxB-wF81V","executionInfo":{"status":"ok","timestamp":1710052879965,"user_tz":-540,"elapsed":2,"user":{"displayName":"Leejeong lenovojeong2000","userId":"01871568169800415408"}},"outputId":"ccbeeaa6-4c26-40a4-9bc2-1e0564995d8a"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[ 1.,  1.,  1.],\n","        [ 1., 23.,  1.]], dtype=torch.float64)\n","[[ 0.9508812  0.9586813  0.4898774]\n"," [ 0.8068895 17.         0.1613965]]\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Auk1oHf0GCsv"},"execution_count":null,"outputs":[]}]}