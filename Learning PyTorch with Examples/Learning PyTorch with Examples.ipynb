{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f2369e50-878a-434a-95a0-1cadd816cdd4",
   "metadata": {},
   "source": [
    "### Learning PyTorch with Examples\n",
    "* 본질적으로, PyTorch에는 두 가지 주요한 특징이 있음.\n",
    "  + NumPy와 유사하지만 GPU 상에서 실행 가능한 n-차원 텐서(Tensor)\n",
    "  + 신경망을 구성하고 학습하는 과정에서의 자동 미분(Automatic differentiation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2abb1c8-bba8-4418-8450-4da3fca75340",
   "metadata": {},
   "source": [
    "* 이 튜토리얼에서는 3차 다항식(third order polynomial)을 사용하여 $ y = sin(x) $ 에 근사(fit)하는 문제를 다뤄보겠음.\n",
    "* 신경망은 4개의 매개변수를 가지며, 정답과 신경망이 예측한 결과 사이의 유클리드 거리(Euclidean distance)를 최소화하여 임의의 값을 근사할 수 있도록 경사하강법(gradient descent)을 사용하여 학습하겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9673ab9-f64f-4d29-a5e3-36bbc97fcc68",
   "metadata": {},
   "source": [
    "### Tensors\n",
    "* Pytorch 를 소개하기 전에, 먼저 NumPy를 사용하여 신경망을 구성해보겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8740ed2-b208-4724-b5e1-3485346590f8",
   "metadata": {},
   "source": [
    "* NumPy는 n-차원 배열 객체와 이러한 배열들을 조작하기 위한 다양한 함수들을 제공함.\n",
    "* NumPy는 과학 분야의 연산을 위한 포괄적인 프레임워크(generic framework).\n",
    "* NumPy는 연산 그래프(computation graph)나 딥러닝, 변화도(gradient)에 대해서는 알지 못함.\n",
    "* 하지만 NumPy 연산을 사용하여 신경망의 순전파 단계와 역전파 단계를 직접 구현함으로써, 3차 다항식이 사인(sine) 함수에 근사하도록 만들 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "943920e5-98eb-47ff-ab7d-d7eb116d7ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 776.5582805896399\n",
      "199 526.9018247860611\n",
      "299 358.77147722890703\n",
      "399 245.44044847389534\n",
      "499 168.9754285518986\n",
      "599 117.33384959768229\n",
      "699 82.42223756241053\n",
      "799 58.796557119144616\n",
      "899 42.79167617046276\n",
      "999 31.937842025484752\n",
      "1099 24.569248393264303\n",
      "1199 19.561253106650362\n",
      "1299 16.15382009479196\n",
      "1399 13.832793737820761\n",
      "1499 12.249992751781487\n",
      "1599 11.169382417491871\n",
      "1699 10.430780494499139\n",
      "1799 9.925362065059526\n",
      "1899 9.579111334333518\n",
      "1999 9.341630904054682\n",
      "Result: y = 0.018773728364737213 + 0.8426577425919265 x + -0.0032387805391055915 x^2 + -0.09132719063377252 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding : utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성함\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화함\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계 : 예측값 y를 계산함\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실 (loss)를 계산하고 출력함\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파함\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신함\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4f581d-6ac4-47da-b640-a262b7d69e77",
   "metadata": {},
   "source": [
    "### Pytorch : Tensors\n",
    "* NumPy는 훌륭한 프레임워크지만, GPU를 사용하여 수치 연산을 가속화할 수는 없음.\n",
    "* 현대의 심층 신경망에서 GPU는 종종 50배 또는 그 이상의 속도 향상을 제공하기 때문에, 안타깝게도 NumPy는 현대의 딥러닝에는 충분치 않음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21bb368-272d-4745-894b-53402fa8d89f",
   "metadata": {},
   "source": [
    "* 이번에는 PyTorch의 가장 핵심적인 개념인 **텐서(Tensor)** 에 대해서 알아보겠음.\n",
    "* PyTorch 텐서(Tensor)는 개념적으로 NumPy 배열과 동일함.\n",
    "* 텐서(Tensor)는 n-차원 배열이며, PyTorch는 이러한 텐서들의 연산을 위한 다양한 기능들을 제공함.\n",
    "* NumPy 배열처럼 PyTorch Tensor는 딥러닝이나 연산 그래프, 변화도는 알지 못하며, 과학적 분야의 연산을 위한 포괄적인 도구임.\n",
    "* 텐서는 연산 그래프와 변화도를 추적할 수도 있지만, 과학적 연산을 위한 일반적인 도구로도 유용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c7f9a8-af69-43a5-818b-639a232e6ff7",
   "metadata": {},
   "source": [
    "* 또한 NumPy와는 다르게, PyTorch 텐서는 GPU를 사용하여 수치 연산을 가속할 수 있음.\n",
    "* PyTorch 텐서를 GPU에서 실행하기 위해서는 단지 적절한 장치를 지정해주기만 하면 됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f72096-e9ee-4dc1-a95f-3d2c5c6fcf31",
   "metadata": {},
   "source": [
    "* 여기에서는 PyTorch 텐서를 사용하여 3차 다항식을 사인(sine) 함수에 근사해보겠음.\n",
    "* 위의 NumPy 예제에서와 같이 신경망의 순전파 단계와 역전파 단계는 직접 구현하겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5dc6986c-288a-4b89-81c1-8c16dbf2cb02",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "torch.cuda.is_available() # GPU 사용 가능한지 여부 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "53687dd1-54d9-45d8-9210-88852e9df856",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 937.6567993164062\n",
      "199 628.136962890625\n",
      "299 421.96893310546875\n",
      "399 284.58001708984375\n",
      "499 192.98141479492188\n",
      "599 131.88067626953125\n",
      "699 91.1024398803711\n",
      "799 63.87183380126953\n",
      "899 45.67750549316406\n",
      "999 33.51333999633789\n",
      "1099 25.37555694580078\n",
      "1199 19.927776336669922\n",
      "1299 16.278249740600586\n",
      "1399 13.831618309020996\n",
      "1499 12.190160751342773\n",
      "1599 11.088027954101562\n",
      "1699 10.347424507141113\n",
      "1799 9.849325180053711\n",
      "1899 9.514020919799805\n",
      "1999 9.28811264038086\n",
      "Result: y = 0.01316809467971325 + 0.8394556641578674 x + -0.0022717139218002558 x^2 + -0.09087172150611877 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding : utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"CPU\") # CPU 에서 실행하기 위한 코드\n",
    "device = torch.device(\"cuda:0\") # GPU 에서 실행하기 위한 코드 (나는 GPU로!)\n",
    "\n",
    "# 무작위로 입력과 출력 데이터를 생성함\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 무작위로 가중치를 초기화함\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # 순전파 단계 : 예측값 y를 계산함\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 손실 (loss)를 계산하고 출력함\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # 손실에 따른 a, b, c, d의 변화도(gradient)를 계산하고 역전파함\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # 가중치를 갱신함\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd69d7-b38d-4475-a42e-82d462eb14b9",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "### Pytorch : Tensors and autograd\n",
    "* 위의 예제들에서는 신경망의 순전파 단계와 역전파 단계를 직접 구현해보았음.\n",
    "* 작은 2계층 (2-layer) 신경망에서는 역전파 단계를 직접 구현하는 것이 큰일이 아니지만, 복잡한 대규모 신경망에서는 매우 아슬아슬한 일일 것임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6d8d7d-3331-48f0-91d0-bac881e27b15",
   "metadata": {},
   "source": [
    "* 다행히도, 자동 미분을 사용하여 신경망의 역전파 단계 연산을 자동화할 수 있음.\n",
    "* PyTorch의 **autograd** 패키지는 정확히 이런 기능을 제공함.\n",
    "* Autograd를 사용하면, 신경망의 순전파 단계에서 **연산 그래프(computational graph)**를 정의하게 됨.\n",
    "* 이 그래프의 노드(node)는 텐서(tensor)이고, 엣지(edge)는 입력 텐서로부터 출력 텐서를 만들어내는 함수가 됨.\n",
    "* 그래프를 통해 역전파를 하게 되면 변화도를 쉽게 계산할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405cbf8-a324-4f0c-abd5-fd2bb500bb06",
   "metadata": {},
   "source": [
    "* 이는 복잡하게 들리겠지만, 실제로 사용하는 것은 매우 간단함.\n",
    "* 각 텐서는 연산그래프에서 노드로 표현됨.\n",
    "* 만약 x 가 x.requires_grad=True 인 텐서라면 x.grad 어떤 스칼라 값에 대한 x 의 변화도를 갖는 또 다른 텐서임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a819b3f-0eca-4290-a5eb-0d795b92b21e",
   "metadata": {},
   "source": [
    "* 여기서는 PyTorch 텐서와 autograd를 사용하여 3차 다항식을 사인파(sine wave)에 근사하는 예제를 구현해보겠음.\n",
    "* 이제 더 이상 신경망의 역전파 단계를 직접 구현할 필요가 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c715e7a7-473b-44ed-bf6c-0385879db4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 248488.015625\n",
      "199 248488.015625\n",
      "299 248488.015625\n",
      "399 248488.015625\n",
      "499 248488.015625\n",
      "599 248488.015625\n",
      "699 248488.015625\n",
      "799 248488.015625\n",
      "899 248488.015625\n",
      "999 248488.015625\n",
      "1099 248488.015625\n",
      "1199 248488.015625\n",
      "1299 248488.015625\n",
      "1399 248488.015625\n",
      "1499 248488.015625\n",
      "1599 248488.015625\n",
      "1699 248488.015625\n",
      "1799 248488.015625\n",
      "1899 248488.015625\n",
      "1999 248488.015625\n",
      "Result: y = -0.5007075667381287 + 1.0118036270141602 x + -2.322666883468628 x^2 + -0.3979623317718506 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding : utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPU 에서 실행함\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성함\n",
    "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를\n",
    "# 계산할 필요가 없음을 나타냄\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서를 생성함, 3차 다항식이므로 4개의 가중치가 필요함\n",
    "# y = a + b x + c x^2 + d x^3\n",
    "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n",
    "# 있음을 나타냄\n",
    "a = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 1e-16\n",
    "for t in range(2000):\n",
    "    # 순전파 단계 : 텐서들 간의 연산을 사용하여 예측값 y를 계산함\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # 텐서들간의 연산을 사용하여 손실(loss)을 계산하고 출력함\n",
    "    # 이 때 손실은 (1, ) shape을 갖는 텐서임\n",
    "    # loss.item() 으로 손실이 갖고 있는 스칼라 값을 가져올 수 있음.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd 를 사용하여 역전파 단계를 계산함. 이는 requires_grad=True를 갖는\n",
    "    # 모든 텐서들에 대한 손실의 변화도를 계산함.\n",
    "    # 이후 a.grad와 b.grad, c.grad, d.grad는 각각 a, b, c, d에 대한 손실의 변화도를\n",
    "    # 갖는 텐서가 됨.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 직접 갱신함.\n",
    "    # torch.no_grad()로 감싸는 이유는, 가중치들이 requires_grad=True 지만\n",
    "    # autograd에서는 이를 추적하지 않을 것이기 때문.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듦.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d06af2-e9ed-4580-a006-e0646ca065e3",
   "metadata": {},
   "source": [
    "### PyTorch: Defining new autograd functions\n",
    "* 내부적으로, autograd의 기본(primitive) 연산자는 실제로 텐서를 조작하는 2개의 함수.\n",
    "* **forward** 함수는 입력 텐서로부터 출력 텐서를 계산함.\n",
    "* **backward** 함수는 어떤 스칼라 값에 대한 출력 텐서의 변화도(gradient)를 전달받고, 동일한 스칼라 값에 대한 입력 텐서의 변화도를 계산함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c258c68-1107-45d6-a040-36053770c8b3",
   "metadata": {},
   "source": [
    "* PyTorch에서 torch.autograd.Function 의 하위클래스(subclass)를 정의하고 forward 와 backward 함수를 구현함으로써 사용자 정의 autograd 연산자를 손쉽게 정의할 수 있음.\n",
    "* 그 후, 인스턴스(instance)를 생성하고 이를 함수처럼 호출하고, 입력 데이터를 갖는 텐서를 전달하는 식으로 새로운 autograd 연산자를 사용할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17ce9d-0bbf-4cbd-8f90-a0651c126cb6",
   "metadata": {},
   "source": [
    "* 이 예제에서는 $ y = a + bx + cx^2 + dx^3 $ 대신 $ y = a + bP_3(c + dx) $ 로 모델을 정의함.\n",
    "* 여기서 $ P_3(x) = \\frac{1}{2} (5x^3 - 3x) $ 은 3차 르장드르 다항식(Legendre polynomial).\n",
    "* $ P_3 $의 순전파와 역전파 연산을 위한 새로운 autograd Function을 작성하고, 이를 사용하여 모델을 구현함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20c5fdd4-a234-41b2-86e6-b290a27eed36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03520202636719\n",
      "499 50.978515625\n",
      "599 37.40313720703125\n",
      "699 28.20686912536621\n",
      "799 21.973186492919922\n",
      "899 17.745729446411133\n",
      "999 14.877889633178711\n",
      "1099 12.931766510009766\n",
      "1199 11.610918998718262\n",
      "1299 10.714248657226562\n",
      "1399 10.105474472045898\n",
      "1499 9.692106246948242\n",
      "1599 9.411375045776367\n",
      "1699 9.220745086669922\n",
      "1799 9.091285705566406\n",
      "1899 9.003361701965332\n",
      "1999 8.943639755249023\n",
      "Result: y = -1.765793067320942e-11 + -2.208526849746704 * P3(9.924167737596079e-11 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding : utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    # torch.autograd.Fucntion 을 상속받아 사용자 정의 autograd Function을 구현하고,\n",
    "    # 텐서 연산을 하는 순전파 단계와 역전파 단계를 구현해보겠음.\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        # 순전파 단계에서는 입력을 갖는 텐서를 받아 출력을 갖는 텐서를 반환함.\n",
    "        # ctx는 컨텍스트 객체(context object)로 역전파 연산을 위한 정보 저장에 사용함.\n",
    "        # ctx.save_for_backward 메소드를 사용하여 역전파 단계에서 사용할 어떤 객체도\n",
    "        # 저장(cache)해 둘 수 있음.\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # 역전파 단계에서는 출력에 대한 손실(loss)의 변화도(gradient)를 갖는 텐서를 받고,\n",
    "        # 입력에 대한 손실의 변화도를 계산해야 함.\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")\n",
    "device = torch.device(\"cuda:0\") # GPU 에서 실행하기 위함.\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성함.\n",
    "# requires_grad=False가 기본값으로 설정되어 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할\n",
    "# 필요가 없음을 나타냄.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 가중치를 갖는 임의의 텐서를 생성함.\n",
    "# 3차 다항식이므로 4개의 가중치가 필요함.\n",
    "# y = a + b * P3(c + d * x)\n",
    "# 이 가중치들이 수렴(convergence)하기 위해서는 정답으로부터 너무 멀리 떨어지지 않은 값으로\n",
    "# 초기화가 되어야 함.\n",
    "# requires_grad=True로 설정하여 역전파 단계 중에 이 텐서들에 대한 변화도를 계산할 필요가\n",
    "# 있음을 나타냄.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # 사용자 정의 Function을 적용하기 위해 Fucntion.apply 메소드를 사용함.\n",
    "    # 여기에 'P3'이라고 이름을 붙임.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # 순전파 단계 : 연산을 하여 예측값 y를 계산함.\n",
    "    # 사용자 정의 autograd 연산을 사용하여 P3를 계산함.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # 손실을 계산하고 출력함.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # autograd를 사용하여 역전파 단계를 계산함.\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법(gradient descent)을 사용하여 가중치를 갱신함.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # 가중치 갱신 후에는 변화도를 직접 0으로 만듦.\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc55ff4f-ada5-4b39-9b5a-09ab14c4627b",
   "metadata": {},
   "source": [
    "### nn Module\n",
    "### PyTorch: nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac1fba8-00be-4f20-a02e-3c4d726f5b9e",
   "metadata": {},
   "source": [
    "* 연산 그래프와 autograd는 복잡한 연산자를 정의하고 도함수(derivative)를 자동으로 계산하는 매우 강력한 패러다임(paradigm)임.\n",
    "* 하지만 대규모 신경망에서는 autograd 그 자체만으로는 너무 저수준(low-level)일 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10e85531-39a5-4f11-9f09-247c0c8cac67",
   "metadata": {},
   "source": [
    "* 신경망을 구성하는 것을 종종 연산을 **계층(layer)**에 배열(arrange)하는 것으로 생각하는데, 이 중 일부는 학습 도중 최적화가 될 **학습 가능한 매개변수**를 갖고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cae0273-4ecc-4a4b-ab80-5f76b68caeaa",
   "metadata": {},
   "source": [
    "* 텐서플로우(Tensorflow)에서는, Keras와 Tensorflow-Slim, TFLearn 같은 패키지들이 연산 그래프를 고수준(high-level)으로 추상화(abstraction)하여 제공하므로 신경망을 구축하는데 유용함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c5ca84-6281-454b-807b-caf2df7c11a6",
   "metadata": {},
   "source": [
    "* 파이토치(PyTorch)에서는 nn 패키지가 동일한 목적으로 제공됨.\n",
    "* nn 패키지는 신경망 계층(layer)과 거의 비슷한 **Module** 의 집합을 정의함.\n",
    "* Module은 입력 텐서를 받고 출력 텐서를 계산하는 한편, 학습 가능한 매개변수를 갖는 텐서들을 내부 상태(internal state)로 가짐.\n",
    "* nn 패키지는 또한 신경망을 학습시킬 때 주로 사용하는 유용한 손실 함수(loss function)들도 정의하고 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151255ce-d23b-4fdf-8800-cf0021459d80",
   "metadata": {},
   "source": [
    "* 이 예제에서는 nn 패키지를 사용하여 다항식 모델을 구현해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "010f2383-22e7-42f1-af6f-e6e344e62876",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 721.7434692382812\n",
      "199 483.88482666015625\n",
      "299 325.5351257324219\n",
      "399 220.07228088378906\n",
      "499 149.8014373779297\n",
      "599 102.95714569091797\n",
      "699 71.71416473388672\n",
      "799 50.865570068359375\n",
      "899 36.94554138183594\n",
      "999 27.64630126953125\n",
      "1099 21.430177688598633\n",
      "1199 17.272315979003906\n",
      "1299 14.48936653137207\n",
      "1399 12.62541389465332\n",
      "1499 11.376047134399414\n",
      "1599 10.538021087646484\n",
      "1699 9.9754638671875\n",
      "1799 9.59749984741211\n",
      "1899 9.343368530273438\n",
      "1999 9.172338485717773\n",
      "Result: y = -0.011105677112936974 + 0.8415225744247437 x + 0.001915913773700595 x^2 + -0.09116571396589279 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 이 예제에서, 출력 y는 (x, x^2, x^3)의 선형 함수이므로, 선형 계층 신경망으로 간주\n",
    "# (x, x^2, x^3)를 위한 텐서를 준비\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# 위 코드에서, x.unsqueeze(-1)은 (2000, 1)의 shape을, p는 (3,)의 shape을 가지므로,\n",
    "# 이 경우 브로드캐스트(broadcast)가 적용되어 (2000, 3)의 shape을 갖는 텐서를 얻음\n",
    "\n",
    "# nn 패키지를 사용하여 모델을 순차적 계층(sequence of layers)으로 정의\n",
    "# nn.Sequential은 다른 Module을 포함하는 Module로, 포함되는 Module들을 순차적으로 적용하여 출력을 생성\n",
    "# 각각의 Linear Module은 선형 함수(linear function)를 사용하여 입력으로부터\n",
    "# 출력을 계산하고, 내부 Tensor에 가중치와 편향을 저장\n",
    "# Flatten 계층은 선형 계층의 출력을 `y` 의 shape과 맞도록(match) 1D 텐서로 핌 (flatten).\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "\n",
    "# 또한 nn 패키지에는 주로 사용되는 손실 함수(loss function)들에 대한 정의도 포함되어 있음\n",
    "# 여기에서는 평균 제곱 오차(MSE; Mean Squared Error)를 손실 함수로 사용함\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "\n",
    "    # 순전파 단계: x를 모델에 전달하여 예측값 y를 계산합니다. Module 객체는 __call__ 연산자를 \n",
    "    # 덮어써서(override) 함수처럼 호출할 수 있도록 합니다. 이렇게 함으로써 입력 데이터의 텐서를 Module에 전달하여\n",
    "    # 출력 데이터의 텐서를 생성\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 손실을 계산하고 출력\n",
    "    # 예측한 y와 정답인 y를 갖는 텐서들을 전달하고,\n",
    "    # 손실 함수는 손실(loss)을 갖는 텐서를 반환\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계를 실행하기 전에 변화도(gradient)를 0으로 만듦\n",
    "    model.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 학습 가능한 모든 매개변수에 대해 손실의 변화도를 계산함\n",
    "    # 내부적으로 각 Module의 매개변수는 requires_grad=True일 때 텐서에 저장되므로,\n",
    "    # 아래 호출은 모델의 모든 학습 가능한 매개변수의 변화도를 계산하게 됨\n",
    "    loss.backward()\n",
    "\n",
    "    # 경사하강법을 사용하여 가중치를 갱신\n",
    "    # 각 매개변수는 텐서이므로, 이전에 했던 것처럼 변화도에 접근할 수 있음\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad\n",
    "\n",
    "# list의 첫번째 항목에 접근하는 것처럼 `model` 의 첫번째 계층(layer)에 접근할 수 있음\n",
    "linear_layer = model[0]\n",
    "\n",
    "# 선형 계층에서, 매개변수는 `weights` 와 `bias` 로 저장됨\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08defe5b-6013-49c6-aaea-9ca9199df0e8",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "* 지금까지는 torch.no_grad() 로 학습 가능한 매개변수들을 갖는 텐서들을 직접 조작하여 모델의 가중치(weight)를 갱신함.\n",
    "* 이것은 확률적 경사하강법(SGD: Stochastic Gradient Descent)와 같은 간단한 최적화 알고리즘에서는 크게 부담이 되진 않지만, 실제로 신경망을 학습할 때는 AdaGrad, RMSProp, Adam 등과 같은 더 정교한 옵티마이저(optimizer)를 사용하곤 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f729cc78-289f-46ef-b700-b7594721f480",
   "metadata": {},
   "source": [
    "* PyTorch의 optim 패키지는 최적화 알고리즘에 대한 아이디어를 추상화하고 일반적으로 사용하는 최적화 알고리즘의 구현체(implementation)를 제공함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651db0a7-8691-4a55-82f7-e9347c0d2ee7",
   "metadata": {},
   "source": [
    "* 이 예제에서는 지금까지와 같이 nn 패키지를 사용하여 모델을 정의하지만, 모델을 최적화할 때는 optim 패키지가 제공하는 RMSProp 알고리즘을 사용함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "288288a1-4613-4fcc-9c34-1fca30df0ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3402.990234375\n",
      "199 1438.7095947265625\n",
      "299 673.4249267578125\n",
      "399 365.02899169921875\n",
      "499 237.97328186035156\n",
      "599 152.7903289794922\n",
      "699 87.22942352294922\n",
      "799 43.701175689697266\n",
      "899 20.209787368774414\n",
      "999 11.027052879333496\n",
      "1099 8.988139152526855\n",
      "1199 9.138055801391602\n",
      "1299 8.827688217163086\n",
      "1399 8.849699974060059\n",
      "1499 8.940855026245117\n",
      "1599 8.95499038696289\n",
      "1699 8.895038604736328\n",
      "1799 8.89332103729248\n",
      "1899 8.91107177734375\n",
      "1999 8.915462493896484\n",
      "Result: y = -0.00019108649576082826 + 0.857255220413208 x + -0.00019188056467100978 x^2 + -0.09281580150127411 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 입력 텐서 (x, x^2, x^3)를 준비\n",
    "p = torch.tensor([1, 2, 3])\n",
    "xx = x.unsqueeze(-1).pow(p)\n",
    "\n",
    "# nn 패키지를 사용하여 모델과 손실 함수를 정의\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(3, 1),\n",
    "    torch.nn.Flatten(0, 1)\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# optim 패키지를 사용하여 모델의 가중치를 갱신할 optimizer를 정의\n",
    "# 여기서는 RMSprop을 사용\n",
    "# optim 패키지는 다른 다양한 최적화 알고리즘을 포함하고 있음\n",
    "# RMSprop 생성자의 첫번째 인자는 어떤 텐서가 갱신되어야 하는지를 알려줌\n",
    "learning_rate = 1e-3\n",
    "optimizer = torch.optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산\n",
    "    y_pred = model(xx)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 역전파 단계 전에, optimizer 객체를 사용하여 (모델의 학습 가능한 가중치인) 갱신할\n",
    "    # 변수들에 대한 모든 변화도(gradient)를 0으로 만듦\n",
    "    # 이렇게 하는 이유는 기본적으로 \n",
    "    # .backward()를 호출할 때마다 변화도가 버퍼(buffer)에 (덮어쓰지 않고) 누적되기 때문\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 역전파 단계: 모델의 매개변수들에 대한 손실의 변화도를 계산\n",
    "    loss.backward()\n",
    "\n",
    "    # optimizer의 step 함수를 호출하면 매개변수가 갱신\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "linear_layer = model[0]\n",
    "print(f'Result: y = {linear_layer.bias.item()} + {linear_layer.weight[:, 0].item()} x + {linear_layer.weight[:, 1].item()} x^2 + {linear_layer.weight[:, 2].item()} x^3')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49ca69c7-d113-4911-ba1c-506abea06314",
   "metadata": {},
   "source": [
    "### PyTorch: Custom nn Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf26bb36-6606-443b-a6be-9564ba909491",
   "metadata": {},
   "source": [
    "* 때때로 기존 Module의 구성(sequence)보다 더 복잡한 모델을 구성해야 할때가 있음\n",
    "* 이러한 경우에는 nn.Module 의 하위 클래스(subclass)로 새로운 Module 을 정의하고, 입력 텐서를 받아 다른 모듈 및 autograd 연산을 사용하여 출력 텐서를 만드는 forward 를 정의함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f7613-1161-47bb-b3b3-61bd797ab439",
   "metadata": {},
   "source": [
    "* 이 예제에서는 3차 다항식을 사용자 정의 Module 하위 클래스(subclass)로 구현해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74d4a2e1-78f7-4fc4-9150-0c217bcc4c8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 3297.433837890625\n",
      "199 2219.85400390625\n",
      "299 1496.6529541015625\n",
      "399 1010.8972778320312\n",
      "499 684.3565063476562\n",
      "599 464.6562805175781\n",
      "699 316.7076721191406\n",
      "799 216.9860382080078\n",
      "899 149.7073211669922\n",
      "999 104.27252197265625\n",
      "1099 73.55868530273438\n",
      "1199 52.77511978149414\n",
      "1299 38.696510314941406\n",
      "1399 29.149606704711914\n",
      "1499 22.66864776611328\n",
      "1599 18.264225006103516\n",
      "1699 15.267695426940918\n",
      "1799 13.226666450500488\n",
      "1899 11.834925651550293\n",
      "1999 10.884824752807617\n",
      "Result: y = -0.035115379840135574 + 0.8265110850334167 x + 0.006057987455278635 x^2 + -0.08903046697378159 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class Polynomial3(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        생성자에서 4개의 매개변수를 생성(instantiate)하고, 멤버 변수로 지정\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        순전파 함수에서는 입력 데이터의 텐서를 받고 출력 데이터의 텐서를 반환해야 함\n",
    "        텐서들 간의 임의의 연산뿐만 아니라, 생성자에서 정의한 Module을 사용할 수 있음\n",
    "        \"\"\"\n",
    "        return self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있음\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3'\n",
    "\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 위에서 정의한 클래스로 모델을 생성\n",
    "model = Polynomial3()\n",
    "\n",
    "# 손실 함수와 optimizer를 생성\n",
    "# SGD 생성자에 model.paramaters()를 호출해주면\n",
    "# 모델의 멤버 학습 가능한 (torch.nn.Parameter로 정의된) 매개변수들이 포함됨\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-6)\n",
    "for t in range(2000):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14db4e03-6186-4b37-9f56-fb9e15128e7d",
   "metadata": {},
   "source": [
    "### PyTorch: Control Flow + Weight Sharing\n",
    "* 동적 그래프와 가중치 공유의 예를 보이기 위해, 매우 이상한 모델을 구현해보겠음.\n",
    "* 각 순전파 단계에서 3 ~ 5 사이의 임의의 숫자(random number)를 선택하여 다차항들에서 사용하고, 동일한 가중치를 여러번 재사용하여 4차항과 5차항을 계산함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516c7b2c-c046-4489-8e84-c38c5337c851",
   "metadata": {},
   "source": [
    "* 이 모델에서는 일반적인 Python 제어 흐름을 사용하여 반복(loop)을 구현할 수 있으며, 순전파 단계를 정의할 때 동일한 매개변수를 여러번 재사용하여 가중치 공유를 구현할 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e8e8d4-c273-4c24-a8ac-6f8defd969c8",
   "metadata": {},
   "source": [
    "* 이러한 모델을 Module을 상속받는 하위클래스로 간단히 구현해보겠음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7149965c-8fb7-4df9-b8ee-f2820dff82a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1999 61.92776107788086\n",
      "3999 34.54628372192383\n",
      "5999 19.4853572845459\n",
      "7999 13.573152542114258\n",
      "9999 11.063141822814941\n",
      "11999 9.801002502441406\n",
      "13999 9.059605598449707\n",
      "15999 8.835355758666992\n",
      "17999 8.945249557495117\n",
      "19999 8.870192527770996\n",
      "21999 8.85826301574707\n",
      "23999 8.630354881286621\n",
      "25999 8.825692176818848\n",
      "27999 8.834794998168945\n",
      "29999 8.859306335449219\n",
      "Result: y = 0.0013924038503319025 + 0.8570119142532349 x + -0.0007548669818788767 x^2 + -0.09372647106647491 x^3 + 0.0001253405207535252 x^4 ? + 0.0001253405207535252 x^5 ?\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import random\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        생성자에서 5개의 매개변수를 생성(instantiate)하고 멤버 변수로 지정\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.a = torch.nn.Parameter(torch.randn(()))\n",
    "        self.b = torch.nn.Parameter(torch.randn(()))\n",
    "        self.c = torch.nn.Parameter(torch.randn(()))\n",
    "        self.d = torch.nn.Parameter(torch.randn(()))\n",
    "        self.e = torch.nn.Parameter(torch.randn(()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        모델의 순전파 단계에서는 무작위로 4, 5 중 하나를 선택한 뒤 매개변수 e를 재사용하여\n",
    "        이 차수들의의 기여도(contribution)를 계산\n",
    "\n",
    "        각 순전파 단계는 동적 연산 그래프를 구성하기 때문에, 모델의 순전파 단계를 정의할 때\n",
    "        반복문이나 조건문과 같은 일반적인 Python 제어-흐름 연산자를 사용할 수 있음\n",
    "\n",
    "        여기에서 연산 그래프를 정의할 때 동일한 매개변수를 여러번 사용하는 것이 완벽히 안전하다는\n",
    "        것을 알 수 있음\n",
    "        \"\"\"\n",
    "        y = self.a + self.b * x + self.c * x ** 2 + self.d * x ** 3\n",
    "        for exp in range(4, random.randint(4, 6)):\n",
    "            y = y + self.e * x ** exp\n",
    "        return y\n",
    "\n",
    "    def string(self):\n",
    "        \"\"\"\n",
    "        Python의 다른 클래스(class)처럼, PyTorch 모듈을 사용해서 사용자 정의 메소드를 정의할 수 있음\n",
    "        \"\"\"\n",
    "        return f'y = {self.a.item()} + {self.b.item()} x + {self.c.item()} x^2 + {self.d.item()} x^3 + {self.e.item()} x^4 ? + {self.e.item()} x^5 ?'\n",
    "\n",
    "\n",
    "# 입력값과 출력값을 갖는 텐서들을 생성\n",
    "x = torch.linspace(-math.pi, math.pi, 2000)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# 위에서 정의한 클래스로 모델을 생성\n",
    "model = DynamicNet()\n",
    "\n",
    "# 손실 함수와 optimizer를 생성\n",
    "# 이 이상한 모델을 순수한 확률적 경사하강법(SGD; Stochastic Gradient Descent)으로\n",
    "# 학습하는 것은 어려우므로, 모멘텀(momentum)을 사용합니다.\n",
    "criterion = torch.nn.MSELoss(reduction='sum')\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-8, momentum=0.9)\n",
    "for t in range(30000):\n",
    "    # 순전파 단계: 모델에 x를 전달하여 예측값 y를 계산\n",
    "    y_pred = model(x)\n",
    "\n",
    "    # 손실을 계산하고 출력합니다.\n",
    "    loss = criterion(y_pred, y)\n",
    "    if t % 2000 == 1999:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # 변화도를 0으로 만들고, 역전파 단계를 수행하고, 가중치를 갱신\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(f'Result: {model.string()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "study",
   "language": "python",
   "name": "study"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
